---
layout: lay_post
title: "机器学习算法-回归-最小二乘法(LeastSquare)"
date: 2016-12-31
categories: 回归算法
tags: 机器学习
author: lvyafei
summary:
published: true
---

* 目录
{:toc #meuid}

## 0.概述

对于线性回归的参数求解问题，可以使用迭代法求解，如梯度下降(GD)。也可以使用非迭代法，如正规方程(NormalEquation)。正规方程的本质是最小二乘法的矩阵形式，最小二乘法求解本质是对代价函数求导。
<!-- more -->

## 1.背景

最小二乘法（又称最小平方法）是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小。最小二乘法还可用于曲线拟合。其他一些优化问题也可通过最小化能量或最大化熵用最小二乘法来表达。

《统计学习方法》中提到，回归学习最常用的损失函数是平方损失函数，在此情况下，回归问题可以用著名的最小二乘法来解决。看来最小二乘法果然是机器学习领域最有名和有效的算法之一。

## 2.最小二乘法(LeastSquare)

我们以最简单的一元线性模型来解释最小二乘法。什么是一元线性模型呢？ 监督学习中，如果预测的变量是离散的，我们称其为分类（如决策树，支持向量机等），如果预测的变量是连续的，我们称其为回归。回归分析中，如果只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。对于二维空间线性是一条直线；对于三维空间线性是一个平面，对于多维空间线性是一个超平面。

对于平面中的这n个点，可以使用无数条曲线来拟合。要求样本回归函数尽可能好地拟合这组值。综合起来看，这条直线处于样本数据的中心位置最合理。 选择最佳拟合曲线的标准可以确定为：使总的拟合误差（即总残差）达到最小。有以下三个标准可以选择：

（1）用“残差和最小”确定直线位置是一个途径。但很快发现计算“残差和”存在相互抵消的问题。

（2）用“残差绝对值和最小”确定直线位置也是一个途径。但绝对值的计算比较麻烦。

（3）最小二乘法的原则是以“残差平方和最小”确定直线位置。用最小二乘法除了计算比较方便外，得到的估计量还具有优良特性。这种方法对异常值非常敏感。

## 3.普通最小二乘法(OrdinaryLeastSquare,OLS)

最常用的是普通最小二乘法(OrdinaryLeastSquare,OLS)：所选择的回归模型应该使所有观察值的残差平方和达到最小。

为了方便讲清楚最小二乘法推导过程这里使用，数据集有1…N个数据组成，每个数据由、构成，x表示特征，y为结果；这里将线性回归模型定义为：

![回归模型](/images/算法/最小二乘法/回归模型.png)

平均损失函数定义有：

![平方损失函数](/images/算法/最小二乘法/平方损失函数.png)

要求得L的最小，其关于c与m的偏导数定为0，所以求偏导数，得出后让导数等于0，并对c与m求解便能得到最小的L此时的c与m便是最匹配该模型的；

**关于c偏导数:**

![c偏导数](/images/算法/最小二乘法/c偏导数.png)

**关于m的偏导数:**

![m偏导数](/images/算法/最小二乘法/m偏导数.png)

**令关于c的偏导数等于0，求解：**

![c偏导数为0](/images/算法/最小二乘法/c偏导数为0.png)

**令关于m的偏导数等于0，求解：**

![m偏导数为0](/images/算法/最小二乘法/m偏导数为0.png)

## 4.普通最小二乘法(OrdinaryLeastSquare,OLS)-矩阵形式

**模型变换**

![模型变换](/images/算法/最小二乘法/模型变换.png)

**代价函数**

![代价函数](/images/算法/最小二乘法/代价函数.png)

根据矩阵乘积转置规则损失函数可以进一步化简为：

![转置-代价函数](/images/算法/最小二乘法/转置-代价函数.png)

**偏导数**

![偏导数](/images/算法/最小二乘法/偏导数.png)

## 5.最小二乘法与梯度下降法比较

最小二乘法跟梯度下降法都是通过求导来求损失函数的最小值，那它们有什么区别呢。

**相同**

1.本质相同：两种方法都是在给定已知数据（independent & dependent variables）的前提下对dependent variables算出出一个一般性的估值函数。然后对给定新数据的dependent variables进行估算。

2.目标相同：都是在已知数据的框架内，使得估算值与实际值的总平方差尽量更小（事实上未必一定要使用平方），估算值与实际值的总平方差的公式为：

![平方差](/images/算法/最小二乘法/平方差.png)

其中x(i) 为第i组数据的independent variable，y(i) 为第i组数据的dependent variable，B 为系数向量。

**不同**

1.实现方法和结果不同：最小二乘法是直接对△求导找出全局最小，是非迭代法。而梯度下降法是一种迭代法，先给定一个B ，然后向△下降最快的方向调整B ，在若干次迭代之后找到局部最小。梯度下降法的缺点是到最小点的时候收敛速度变慢，并且对初始点的选择极为敏感，其改进大多是在这两方面下功夫。

参考资料:http://www.cnblogs.com/softlin/p/5815531.html